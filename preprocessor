import librosa
import torch
from encodec import EncodecModel
from encodec.utils import convert_audio

# Initialize the Encodec model
def load_encodec_model():
    model = EncodecModel.encodec_model_24khz()
    model.set_target_bandwidth(6.0)  # Bandwidth affects token compression
    model.eval()
    return model

class MP3Preprocessor():
    def __init__(self, dataset_path, model, segment_duration=15.0, sample_rate=24000):
        self.dataset_path = dataset_path
        if len(self.dataset_path) < 2:
            raise ValueError("Dataset must contain at least two MP3 files.")
        self.model = model
        self.segment_duration = segment_duration
        self.sample_rate = sample_rate

    def load_audio_segment(self, file_path, start_time, duration):
        audio, _ = librosa.load(file_path, sr=self.sample_rate, offset=start_time, duration=duration)
        return torch.tensor(audio).unsqueeze(0)

    def tokenize_audio(self, audio_segment):
        audio_segment = convert_audio(audio_segment, self.sample_rate, 24000, 1)
        with torch.no_grad():
            encoded_frames = self.model.encode(audio_segment.unsqueeze(0))
        tokens = torch.cat([frame[0] for frame in encoded_frames], dim=-1)
        return tokens

    def preprocess_pair(self, song_a_path, song_b_path):
        song_a_audio, _ = librosa.load(song_a_path, sr=self.sample_rate)
        song_a_duration = len(song_a_audio) / self.sample_rate

        song_a_outro = self.load_audio_segment(song_a_path, start_time=song_a_duration - self.segment_duration, duration=self.segment_duration)
        song_a_tokens = self.tokenize_audio(song_a_outro)
        song_a_tokens = song_a_tokens.permute(0, 2, 1)
        
        song_b_intro = self.load_audio_segment(song_b_path, start_time=0.0, duration=self.segment_duration)
        song_b_tokens = self.tokenize_audio(song_b_intro)
        song_b_tokens = song_b_tokens.permute(0, 2, 1)
        
        return song_a_tokens, song_b_tokens

    def generate_batches(self, batch_size=1):
        pairs = list(zip(self.dataset_path[:-1], self.dataset_path[1:]))
        for i in range(0, len(pairs), batch_size):
            batch_input, batch_target = [], []
            for song_a, song_b in pairs[i:i + batch_size]:
                input_seq, target_seq = self.preprocess_pair(song_a, song_b)
                batch_input.append(input_seq.squeeze(0))  # Remove the extra batch dimension
                batch_target.append(target_seq.squeeze(0))  # Remove the extra batch dimension
            yield torch.stack(batch_input), torch.stack(batch_target)
